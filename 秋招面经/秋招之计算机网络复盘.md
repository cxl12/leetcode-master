# 计算机网络方面

* 什么是协议?
  * 协议就是计算机与计算机之间通过网络通信时，事先达成的一种 “约定”。这种“约定”使不同厂商的设备、不同的CPU以及不同操作系统组成的计算机之间，只要遵循相同的网络协议就能够实现通信。

## 计算机网络体系结构

* 什么是协议
  * 协议就是计算机与计算机之间通过网络通信时，事先达成的一种 “约定”。这种“约定”使不同厂商的设备、不同的CPU以及不同操作系统组成的计算机之间，只要遵循相同的网络协议就能够实现通信。

### 七层协议

* 网络层七层
  * 应用层，表示层，会话层，传输层，网络层，数据链路层，物理层
  * 应用层
    * 为操作系统或网络应用程序提供访问网络服务的接口
      * 传输的数据单元是报文：message
      * http协议，ftp协议等
  * 表示层
    * 将应用处理的信息转换为适合网络传输的格式，或将来自下一层的数据转换为上层能够处理的格式。对数据的表示、安全、压缩。
  * 会话层
    * 负责建立和断开通信连接（数据流动的逻辑通路），以及数据的分割等数据传输相关的管理。
  * 传输层
    * 管理网络中的节点之间的数据传输
    * tcp协议：报文段（segment）
    * udp协议：用户数据包（package）
  * 网络层
    * ip协议：数据单位是分组
    * 地址管理与路由选择, 在这一层，数据的单位称为数据包（packet）（路由器）。
  * 数据链路层
    * 数据单元：ip数据报封装成帧（frame)
    * 互连设备之间传送和识别数据帧（交换机）
    * 将网络层交下来的ip数据包封装成帧发送到链路，接收到的帧中取出数据交付给网络层
    * 三大特性
      * 封装成帧：在ip数据报文加上头部和尾部，进行帧定界，规定了帧的数据部分的最大传送长度MTU（ip数据报文）
      * 透明传输：帧的用于分界的控制字符不会在报文内出现，不被数据部分所看到
      * 差错检测：冗余检验CRC
      * 协议
        * ppp协议：点到点通信协议
        * CSMA/CD协议：载波监听多点接入/碰撞检测协议
          * 边发送边监听，如果发现总线上出现了碰撞，则适配器马上停止发送，等待一段时间再发送。
      * 组件
        * 集线器：进行转发bit,同一个 时刻只能有一个站点发数据
      * max地址
        * 局域网上 规定的一种48位全球唯一的地址，即固化在每个计算机的适配器（网卡）的ROM上的地址
        * 用于区分局域网的一种地址
          * 同一台电脑换了适配器，则它虽然地理位置不变，但是已经不属于这个局域网
          * 同一台 电脑由广州带到北京并连接上北京的的某个局域网，虽然地理位置改变了，但是适配器没变，它的局域网地址还是广州的局域网地址一样。
        * max地址又称为硬件地址，在生产适配器时就固化在适配器上，他有过滤功能，当适配器接收到 网络上的一个mac帧，先用硬件检查mac帧的目的地址，不符合就丢弃 该帧
      * 网桥
        * 收到帧后，根据转发表的地址，根据max帧的目的地址进行转发
        * 透明网桥：用自学习算法去进行转发
      * 多接口网桥：交换机
        * 可以同时连接多对接口，是的每一对相互通信的主机可以独占传输媒体那样，无碰撞地传输数据。
  * 物理层
    * 数据的单位称为比特（bit）,（中继器、集线器、还有我们通常说的双绞线也工作在物理层）

  * 工作流程
    * 由上到下，每个分层再处理上层传递的数据附上当前层的协议，向下传输
    * 接收方从下到上传递数据，每个分层对接收到的数据进行首部分离，向上传递

* ip层的ip地址
  * 整个网络的路由器接口都分配一个全球唯一的32位ip地址，用于通信，32位ip地址划分为两个部分，网络号划分子网，主机号区分子网内的主机
    * A类:  8  ： 24
    * B类： 16 :  16
    * C类： 24 :  8
  * ip地址与mac硬件地址
    * max地址是数据链路层 和物理层用的硬件地址，而ip地址是网络层以上用的逻辑地址
    * ip数据包交给数据链路层，ip数据包封装mac帧，在传送过程中使用源地址和目的地址都是 mac硬件地址。在网络层以上都是使用ip地址去通信。
    * ip层 抽象 的互联网只能看到ip数据报（路由传输），局域网的 链路层只能看到mac帧
    * 尽管互联的局域网之间千差万别，但ip层却屏蔽了下层的复杂细节，在ip层抽象一个IP地址，就可以在网络层上使用唯一的ip地址进行通信。
    * 为什么有了mac还需要ip地址呢？
      * 理由1：由于全世界存在着各式各样的网络，他们使用不同的硬件地址。要使这些异构网络能够互相通信就必须进行非常复杂的硬件地址转化工作，因此由用户或用户主机来完成这项工作几乎是不可能的的事。但IP编址就把这个复杂的问题解决了。连接到互联网的主机只需要各自拥有一个唯一的IP地址，他们之间的通信就像连接在同一个网络那么简单方便。
      * :理由2：只拥有MAC地址的话，只有在同一网络区域内，才能进行数据传输，不能跨网络区域。如果想跨网络区域进行数据传递，最现实的方法就是借助ISP提供的网络区域。ISP能提供全球互联的网络——因特网，借助因特网可以传输数据给连接因特网上的机器。
        * 形象说明：
          * 有一个与世隔绝的村子（它就相当于一个局域网）村子的每个人都有自己的名字（名字相当于mac地址），在村子内部，大家可以通过名字直接去物品交换，自给自足。
          * 后来有人发现了外面的世界（互联网），想要于外界通信就要修公路（即申请一个ip地址，申请加入互联网），那么这个村子就用互联网去标识。
          * 如果村子有人参加网购 ，它只用了ip地址 ，那么快递只能到达村子 ，还需要一个mac地址去在村子中去区分每个人。

  * arp协议
    * 用于从网络上使用的IP地址解析出在数据路层使用的硬件地址。
    * 主机在ARP高速缓存中维护一个IP地址到MAC地址的映射表，当 主机A向本局域网发送一个IP数据报给主机B
      * 先查ARP缓存表，找到主机B的mac地址则将其写入mac帧，然后通过局域网发给主机B
      * 如果缓存表中没有主机B的映射 ，就启用arp协议
        * 在本局域网广播一个arp请求（我的ip地址是xxx,我的mac地址是xxx,我先知道ip地址是xxx的主机的 mac地址 ）
        * 本局域网内所有的运行arp协议的主机都会收到该ARP请求分组
        * 主机B的ip地址与ARP请求分组要查询的IP地址一致，就收下该ARP请求分组，并 将自己的mac地址以响应分组的方式回发。
        * 主机A收到响应后，将其写入映射表，发送数据时从映射表找到B主机的mac地址。

  * 子网划分
    * 一个拥有许多物理网络的单位，它的ip地址不够灵活，可以划分为若干个更小的子网，分子网是它内部的事情，这个单位对外还是表现为一个网络
    * 从主机号中划分几位作为子网号
    * 当收到发给本主机的ip数据报，先根据ip地址的网络号找到本单元网络的看路由器，路由器收到ip数据报，按照目的网络号和子网号找到目的子网，最后将ip数据报交给主机号区分的目的主机。
    * 通过子网掩码区分子网号(将子网掩码与目的ip地址按位与，得到目的网络地址)

  * CIDR超网
    * 消除传统的abc类地址，把32位ip地址分为前缀和主机号，前缀用来指明网络，主机号用于区分主机

  * ICMP报文
    * 提高转发ip数据报和提高交付成功的机会
      * 分为差错报告 报文和询问报文
    * 应用
      * ping
        * 测试主机之间的连通性，使用了ICMP回送请求和回送回答报文
        * 在访问某个 服务器前，为了检测连通性，先发四个icmp回送请求报文，如果对方收到就会返回ICMP回送回答报文
      * traceroute
        * 跟踪源点到终点的路径
          * 从源主机向目的主机发送一连串的ip数据报，数据报封装了无法交付udp报文
            * 第一个数据报将ttl设置为1，当p1到达路径的第一个路由器时，路由器先收下然后ttl-1,当ttl=0则丢弃报文，并向源主机发送一个ICMP时间差错报告报文
            * 第二个数据报将ttl设置为2
            * 第k个设置为k
          * 就可以得到它的路径

  * OSPF协议
    * 开放最短路径优先协议（路由交换用）

  * vpn技术
    * 由于ip地址紧缺，一个机构可以申请的全球 IP地址少于需求量，可以使用vpn技术，在本机构内可以使用仅在本机构内 有限的专用ip地址，它可以和全球ip地址重复
    * 两种网络地址的转换NAT
      * 专用网内部发送 数据之前，根据转换表，转换为一个全球ip地址，然后发送给目的主机，收到ip数据报，机构根据转换表转为本地专用ip地址 ，交付给相应的 主机

### 五层协议

* 应用层
* 传输层
* 网络层
* 数据链路层
* 物理层

### TCP/IP协议

* 应用层
* 传输层
* 网际层
* 网络接口层

### TCP/UCP（传输层）

* 什么是socket?
  * socket是应用层与传输层之间的抽象接口，在用户进程与TCP/IP协议之间充当中间人，是计算机进程之间通信的一种方式，通过socket套接字可以实现同一计算机不同进程之间通信，和不同计算机的不同进程之间通信。
  * socket（IP：Port)
    * 同一个ip地址可能有多个不同的连接，端口区分具体的应用程序

* socket编程的使用过程

  * 服务器端
    * 服务器调用socket接口函数socket(int family, int type, int protocol)获取套接字，它像文件描述符，可以像读写文件一样用read/write在网络上收发数据，因为socket有缓冲区，可以像操作管道一下操作socket，套接字对于用户程序而言就是特殊的已打开的文件。
  
    * 为了监听传入的 Internet 连接请求，在创建socket套接字后，需要调用bind函数将进程的ip地址 和 端口绑定到该套接字，服务器程序所监听的网络地址和端口号通常是固定不变的，客户端程序得知服务器程序的地址和端口号后就可以向服务器发起连接，
  
    * 为了随时接受到客户端的连接请求，服务器需要调用循环调用listen(int sockfd, int backlog)去监听socket套接字。典型的服务器程序可以同时服务于多个客户端，当有客户端发起连接时，服务器调用的accept()返回并接受这个连接，如果有大量的客户端发起连接而服务器来不及处理，尚未accept的客户端就处于连接等待状态，listen()声明sockfd处于监听状态，并且最多允许有多个客户端处于连接待状态，如果接收到更多的连接请求就忽略。listen()成功返回0，失败返回-1

    * int accept(int sockfd, struct sockaddr *cliaddr, socklen_t *addrlen)，三次握手完成后，服务器调用accept()接受连接，如果服务器调用accept()时还没有客户端的连接请求，就阻塞等待直到有客户端连接上来。服务器接收到传入的请求后，如果能够接受该请求，服务器必须创建一个新的套接字来接受该请求并建立通讯连接（用于监听的套接字不能用来建立通讯连接），这时，服务器和客户就可以利用建立好的通讯连接传输数据。

  * 客户端
    * 创建socket套接字

    * 由于客户端不需要固定的端口号，因此不必调用bind()，客户端的端口号由内核自动分配。注意，客户端不是不允许调用bind()，只是没有必要调用bind()固定一个端口号，服务器也不是必须调用bind()，但如果服务器不调用bind()，内核会自动给服务器分配监听端口，每次启动服务器时端口号都不一样，客户端要连接服务器就会遇到麻烦。客户端需要调用connect()连接服务器，connect和bind的参数形式一致，区别在于bind的参数是自己的地址，而connect的参数是对方的地址。connect()成功返回0，出错返回-1。
  * 双方建立连接就可以通信

* socket阻塞与非阻塞，同步与异步
  * 同步/异步主要是针对client端
    * 同步：就是在client端发出一个功能调用时，在没有得到结果之前，该调用就不返回
    * 异步：与同步相对，当client端调用发出后，会立即返回，但调用者不能立刻得到结果，而是由其它通知或回调来通知调用者。
  * 阻塞/非阻塞主要针对server端
    * 阻塞调用是指调用结果返回之前，当前线程会被挂起，函数只有在得到结果之后才会返回。
    * 非阻塞则会立刻返回。像Recv接口是阻塞的，RecvFrom非阻塞的。

* socket选项
  * Socket选项就是为满足用户的定制化需求而生的。我们经常遇到的情况包括地址复用、端口复用、读写超时时间、读写缓冲区大小等
  * 读取和设置Socket选项的API包括:
    * getsockopt 获取socket描述符的属性
    * setsockopt 设置socket描述符的属相
    * fcntl、ioctl等；
  * 什么时候设置选项
    * 对于服务器而言，部分socket选项只能在调用listen之前针对监听socket设置才有效。这是因为连接socket只能由accept调用返回，而accept从listen监听队列中获取的连接是已经完成三次握手的，这说明服务器已经想被接受的链接上发送出了TCP同步报文段。但有的socket选项却应该在TCP同步报文段中设置（比如：TCP的最大报文段）。
    * 对于客户端而言，这些socket选项应该在调用connect函数之前设置，因为connect函数调用成功返回后，TCP三握手已经完成。
  * 几个重要的选项
    * SO_REUSEADDR选项
      * 一般来说，一个端口释放后会等待两分钟之后才能再被使用，SO_REUSEADDR是让端口释放后立即就可以被再次使用
      * 场景
        * TCP断开连接四挥手的最后一步，作为主动断开连接的一方有一个time_wait的状态，这个状态一般持续2MSL，但是，在TCP连接没有完全断开之前不允许重新监听是不合理的。因为，我们重新监听的连接，虽然是占用同一个端口，但IP地址不同
        * 当有一个有相同本地地址和端口的socket1处于TIME_WAIT状态时，而你启动的程序的socket2要占用该地址和端口，你的程序就要用到该选项。

    * TIME_WAIT
      * TIME_WAIT状态是TCP协议为了保证全双工连接可靠性设置的
        * 可靠地实现TCP全双工连接的终止
        * 可靠地实现TCP全双工连接的终止

    * SO_REUSEPORT选项
      * 作用效果:端口复用选项
      * 支持多个进程或者线程绑定到同一端口，提高服务器程序的性能
        * )单进程或线程创建socket，并进行listen和accept，接收到连接后创建进程和线程处理连接
        * 单进程或线程创建socket，并进行listen，预先创建好多个工作进程或线程accept()在同一个服务器套接字
    * SO_RCVBUF和SO_SNDBUF
      * 每个socket都有自己的接收/发送缓存。当数据到达，而进程未来得及处理的时候，将被缓存起来。对于TCP来说，接收缓存影响滑动窗口的大小，系统会根据缓存的情况进行流量控制。
      * 利用SO_RCVBUF和SO_SNDBUF这两个选项，我们可以修改缓存的默认大小。在老式的实现中，TCP的接收/发送缓存的默认值是4096，在新的系统中使用了更大的值，一般介于8192到61440；UDP的发送缓存一般是9000左右，接收缓存一般是40000左右。
      * 对TCP的缓存进行设置的时候，需要注意顺序问题。TCP的窗口大小初始值附加在连接建立时发送的SYN包中，因此，对于客户端，设置应当发生在调用connect前，而对于服务器，应当在listen之前（由accept返回的socket继承listening socket的设置）。
      * TCP socket缓存的大小应至少有4个MSS大。这里所说的TCP socket缓存，指的是发送方的发送缓存和接收方的接收缓存。
      * 为了避免缓存空间的浪费，socket的缓存大小应该为MSS的偶数倍。在一些系统中，底层会自动进行圆整，比如若Ethernet的MSS为1460，默认缓存大小为8192，系统会自动圆整为8760(1460*6)。
      * 缓存的大小直接影响socket的效率，大了浪费空间，小了则链路的带宽得不到有效的利用。一般来说，缓存大小至少设置为链路的容量。链路的容量就是带宽时延积，即同一时间内链路上能够容纳的比特数。如果小于这个值，则链路则不是“满”的。

    * SO_RCVLOWAT 和SO_SNDLOWAT 选项
      * 这两个选项分别表示tcp接受和发送缓冲区的低水位标志。它们一般被I/O复用系统调用来判断socket是否可读或可写。
      * 当tcp接收缓冲区中可读数据的总数大于低水位标记时，I/O系统调用将通知应用程序可以从对于的socket读取数据，当TCP发送缓冲区中的空闲空间大于其低水位，将通知应用程序可以往对应的socket上写数据。
      * 默认情况下，这两个选项均为1字节！

    * SO_LINGER
      * 此选项指定函数close对面向连接的协议如何操作（如TCP）。内核缺省close操作是立即返回，如果有数据残留在套接口缓冲区中则系统将试着将这些数据发送给对方。
        * 缺省
        * 立即返回，丢弃数据

    * TCP_NODELAY选项
      * 启用Naggle算法被引入到协议栈，算法思想尽可能发送大块数据，避免网络中充斥着许多小数据块，任意时刻最多只能有一个未被确认的小段。未被确认是指一个数据块发送出去后，没有收到对方发送的ACK确认
      * Nagle算法：
        * 是为了减少广域网的小分组数目，从而减小网络拥塞的出现；
        * 该算法要求一个tcp连接上最多只能有一个未被确认的未完成的小分组，在该分组ack到达之前不能发送其他的小分组，tcp需要收集这些少量的分组，并在ack到来时以一个分组的方式发送出去；其中小分组的定义是小于MSS的任何分组；
        * 该算法的优越之处在于它是自适应的，确认到达的越快，数据也就发哦送的越快；而在希望减少微小分组数目的低速广域网上，则会发送更少的分组；

* TCP 首部各字段的意义和作用
  * TCP首部最小为20字节，这20字节分为5行
  * 源端口和目的端口
  * 序号：对数据包进行标记，用于接收方进行排序
  * 确认号：期待对方下一个报文端的序号，TCP的可靠性的基础。
  * 偏移量：报文段在整个报文的起始位置的偏移量
  * 标志位
    * ACK,RST,SYN,FIN
    * ACK标志为1时，确认号有效
    * RST标志为1表示TCP连接出现严重错误，需要重连
    * SYN为1时，表示是个请求连接报文段，用于同步序号
    * FIN标志表示此报文段的发送方的数据已经发送完毕，要求释放TCP连接。

* 复用和分用的概念
  * 复用
    * 在发送方不同的应用进程都可以使用同一个传输层协议传送数据
  * 分用
    * 接收方的传输层在剥去报文的首部后，能够将这些数据正确交付目的应用程序。

* TCP 和 UDP的区别？ 面向连接的连接的含义？
  * TCP 是传输控制协议
    * 首部有20个字节
    * 发送数据前需要三次握手建立连接
    * 面向字节流
      * 流入进程货从进程中流出的是无格式字节序列
    * ，它提供可靠的面向连接服务：
      * 当客户和服务器彼此交换数据前，必须先在双方之间建立一个TCP连接，之后才能传输数据，在数据传递时，按序发送，有确认重传、检验，拥塞控制，流量控制等功能，实现可靠传输
    * 缺点是：包验证复杂，导致速度比较慢，发送的是字节流数据无边界
    * 场景：适合可靠性比较高的应用

  * UDP 是用户数据报文协议
    * 首部开销只有8个字节
    * 它是无连接的，提供尽最大努力的无连接服务
    * 它是面向报文的，对应用程序交下来的报文添加首部，以原来的样子发送，一次发送一个报文，如果报文太长，则由iP层的协议进行分片。
    * 数据传递时，无序号，确认，重传机制，容易丢包，发送的是用户数据报文，优点是速度快，适合延迟比较敏感的应用。
    * 优点是传输速度快，适用与可靠性比较高的应用

* TCP和UDP的包头
  * tcp包头大小是20个字节
    * 源端口
    * 目的端口
    * 序号
      * 一个TCP连接中传送的字节流的每一个字节都 按照顺序编号，序号表示本报文段所发送的数据的第一个字节的序号
    * 包的确认号
      * 希望收到的下一个报文段的第一个数据字节的序号
    * 偏移量
      * 本报文段的数据起始位置相对于TCP报文段的起始位置的距离。
    * 包长度
    * 窗口
      * 发送本报文的发送方的接收端口，用于告诉对方，从本报文段首部中的确认号算起，接收方目前允许对方发送的数据量，窗口值作为接收方让发送方设置其发送窗口的依据
    * ACK
      * 当ACK=1时，确认好有效，建立连接后的每个报文段都设置为1
      * SYN
        * 在建立连接时的同步序号，SYN = 1时而ACK=0,表示这是一个连接请求报文，对方统一连接则会在相应 报文段中将SYN=1 和ACK=1，表示这是一个连接请求或连接接受报文
      * FIN
        * 用来释放一个连接，当FIN = 1时，表示此报文的发送方的数据已经发送了，要求释放传输连接
      * URG
        * 表明紧急指针，搞死系统此报文有紧急数据，应该尽快发送
      * PSH
        * 当两个应用进程进行交互式通信时，有时在一端的应用程序希望在键入一个命令后立即收到对方的回应，将PSH=1,那么接收方在收到该报文时尽快交付给接收进程，不等缓存满了在往上提交
      * RST
        * 表明TCP连接出现严重错误，必须释放连接，然后再重新建立运输连接，还可以用来拒绝一个非法的报文段或者拒绝打开一个连接。
    * 校验和
      * 检验和字段检验的范围包括首部和数据这两个部分

  * UCP包头8个字节
    * 源端口
    * 目的端口
    * 包长度
    * 校验和

* 以太网包头长度 14个字节
  * 目的地址
  * 源地址
  * 类型
  * wireshark进行抓包分析

* 以太网帧的包头长度
  * 长度时20-60个字节
  * 版本
  * 首部长度
  * 类型
  * 总长度
  * 标志
  * 片偏移量
  * 协议
  * 源地址
  * 目的地址
  * ...

* MSS 与MTU
  * MSS（Maxsinum Segment Size)
    * 最大报文段长度
    * 每一个TCP报文段中的数据字段的最大长度（即TCP报文长度-头部长度）
    * MSS大小选择因素
      * MSS与TCP头部一起组成这个报文段
        * 如果过小则头部大于数据部分，导致传输每个字节的数据的开销变大，从而降低了网络的利用率
        * MSS尽量大，只要不被ip协议分片即可，由于ip报文的传输路径是动态的，导致无法确定最佳的MSS,一般设置为146

* ARQ协议（停止等待协议）
  * 解释
    * 发送方每发送一个分组就停止发送，等待对方的确认，收到确认后才发送下一个分组
    * 如果出现差错
      * 发送方如果超过指定的时间内仍然没有收到确认，就认为刚发的包丢失了，然后进行超时重传
      * 有一个超时计时器 ，如果在超时之前收到确认，则撤销计时器
        * 重传超时时间，怎样设置这个定时器的时间（RTO），从而保证对网络资源最小的浪费。因为若RTO太小，可能有些报文只是遇到拥堵或网络不好延迟较大而已，这样就会造成不必要的重传。太大的话，使发送端需要等待过长的时间才能发现数据丢失，影响网络传输效率。由于不同的网络情况不一样，不可能设置一样的RTO，实际中RTO是根据网络中的RTT（传输往返时间）来自适应调整的。太小，造成不必要的重传，太长则降低通信效率。
  * 注意事项
    * 发送方发送完分组后，主要保存已经发送的分组的副本，收到才删除
    * 分组和确认分组需要编号
    * 超时重传的时间应该比数据在分组传输的 RTT 往返 时间的平均值稍微大一些。
  * 当接收方收到重复分组时
    * 第一个种情况就是丢弃这个重复的分组
    * 第二个就是向发送方放送一个确认（因为发送方重发了分组，可能没有收到上一次的确认）
  * 这种自动重传请求的协议就是ARQ协议
    * 重传的请求是自动进行的，接收方不需要请求发送方去重传某个出错的分组。

* 连续ARQ协议
  * 流水线传输
    * 发送方可连续发送多个分组，不必每发送一个分组就停下等待确认，提高信道的利用率
  * 发送方会维护一个发送窗口，里面有多个分组
  * 当发送方收到一个小序号分组的确认后，窗口就向前移动一位，这是可以发送新的分组
  * 接收方一般采用累积确认的机制，不用对接收到的每个分组都发确认，而是受到多个分组后，对按序到达的最后一个分组发送确认，表示这个位置之前的分组都到达了
  * 如果发送方发了5个分组，中间三个丢失了，发送方只能受到对前面 一个分组的确认，此时需要回退，重发丢失的三个分组。

* TCP怎么实现可靠传输（包重传）
  * 发送方维护一个发送窗口
    * 窗口内的序号是允许发送的序号的报文段
    * 窗口后面的是已经收到确认的报文段
    * 前面是还没发送的报文段的序号
  * 窗口的调节
    * 窗口的大小是根据接受方的接受端口以及网络拥塞情况控制的
    * 维持不动
      * 没有收到新的确认
      * 收到确认后，但对方的接收端口缩小了，所以发送方的发送窗口位置不变
    * 向前移动
      * 收到了新的确认
      * 对方的接收窗口扩大了
  * 滑动窗口内有三个指针p1,p2,p3
    * p2之前是已经发送但未确认
    * p2之后是允许发送但还没发送
    * 只有低序号的报文段按需到达才会向前 移动，不按需到达则先暂存在接收窗口
    * p2之前的报文段在超时计数器超时后重传这部分报文

  * TCP连接中双方都有发送缓存和接收缓存
    * 发送方进程将字节流写入TCP的发送缓存，接收方进程从接收缓存读取字节流
    * 发送缓存
      * 发送进程传送给TCP发送端的数据
      * TCP已经发送但是没有得到确认的数据。
    * 接收缓存
      * 按序到达的，但是还未被接收应用程序读取的数据
      * 未按需到达的数据

  * 总结3点
    * 发送方的窗口值大小是以接收方的接收窗口而定，但也会根据网络拥塞变化而调整
    * TCP通常对不按序到达的数据是先暂存在接收窗口，等到字节流中缺少的字节到达才会交付到上层的进程
    * TCP有累积确认，可以减少传输开销，接收方可以借助要发送的数据报文将确认信息带上，但是为了让发送方产生不必要的重传，确认推迟时间不应超过0.5秒。
　　
  * 超时重传时间的选择
    * 采用自适应算法
      * 记录一个报文段发出的时间，以及收到相应的确认的时间，时间差就是报文的RTT往返时间 ，对其取加权平均值
      * 重传超时时间，怎样设置这个定时器的时间（RTO），从而保证对网络资源最小的浪费。因为若RTO太小，可能有些报文只是遇到拥堵或网络不好延迟较大而已，这样就会造成不必要的重传。太大的话，使发送端需要等待过长的时间才能发现数据丢失，影响网络传输效率。由于不同的网络情况不一样，不可能设置一样的RTO，实际中RTO是根据网络中的RTT（传输往返时间）来自适应调整的。太小，造成不必要的重传，太长则降低通信效率。

  * 选择确认SACK
    * 如果报文段无差错，但是没有按需到达，这是设法只重传确认报文丢失的报文段，这就要通过在TCP报文段头部加SACK，报告收到的不连续的字节块的边界，告诉发送方不用再发这部分数据。

* TCP的流量控制
  * 流量控制就是让发送方的发送速率不要太快，要让接收方来得及接收。
  * 发送方的发送窗口不能超过接收方的接收窗口，单位是字节
    * 一开始连接的时候发送接收窗口给接收方
    * 特殊情况：接收方的接收窗口增大了，通过报文告诉发送放方，而此时该报文丢失，造成发送方等接收方的非零窗口的确认，而接收方也在等接收方的数据，双方造成相互等待的死锁。
      * 通过持续计时器打破死锁，当计时器为0，则发送方发起一个零窗口探测报文段，此时就可以打破僵局

  * TCP报文的发送时机
    * 三种机制
      * TCP维持一个变量，它的大小是MSS,当缓存中的字节流超过MSS字节，则组成一个报文发送
      * 由发送进程知名要求发送一个报文（push操作）
      * 发送方维护一个计时器，移动时间就把缓存装入报文段发送
    * 可以采用推迟发送确认报文，尽量使用捎带确认的方法
    * TCP广泛使用Nagle算法
      * 若发送进程把要发送的数据逐个字节送到tcp缓存，则发送方就把第一个数据字节先发送出去，把后面到达的数据字节都缓存起来，当发送方收到第一个数据字节的确认后，再发送缓存中所有数据装成一个报文段发送出去，同时继续对随后到达的 数据进行缓存。（如果网络拥塞，这样子可以减少所用的带宽），Nagle算法还规定，当到达的数据达到发送窗口大小的一半或报文段的最大长度时，立即发送一个报文，可以有效提高网络的吞吐量。
  * 慢开始
    * 主机开始传输数据的时候，是慢慢增大发送窗口的值，直到拥塞窗口的值，一开始设为1，每次接收一个报文段确认，发送方的拥塞窗口值加倍

* TCP拥塞控制
  * 拥塞：资源的需求超过了资源能够提供的可用部分
  * 拥塞控制
    * 就是防止过多的数据注入到网络中，这样就可以使得网络上的路由器或链路不至于过载，没超过网络的负荷。

  * 发送方维护一个拥塞控制窗口，大小取决于网络的拥塞程度，是动态变化的。发送方让自己的发送窗口等于拥塞窗口。

  * 慢开始算法
    * 当主机开始发送数据时，如果立即把大量数据字节注入到网络，那么就可能引起网络拥塞，所以先探测一下，即由小逐渐增大发送窗口，一开始设置为一个MSS,每经过一个传输轮次将拥塞窗口值加倍
    * 慢启动算法的基本思想是当TCP开始在一个网络中传输数据或发现数据丢失并开始重发时，首先慢慢的对网路实际容量进行试探，避免由于发送了过量的数据而导致阻塞
    * 慢是指一开始cwnd设置为1，在开始时只发送一个报文
    * 慢启动为发送方的TCP增加了另一个窗口：拥塞窗口(congestion window)，记为cwnd。当与另一个网络的主机建立TCP连接时，拥塞窗口被初始化为 1个报文段（即另一端通告的报文段大小）。每收到一个ACK，拥塞窗口就增加一个报文段（cwnd以字节为单位，但是慢启动以报文段大小为单位进行增加）。发送方取拥塞窗口与通告窗口中的最小值作为发送上限。拥塞窗口是发送方使用的流量控制，而通告窗口则是接收方使用的流量控制。发送方开始时发送一个报文段，然后等待 ACK。当收到该ACK时，拥塞窗口从1增加为2，即可以发送两个报文段。当收到这两个报文段的 A C K时，拥塞窗口就增加为4。这是一种指数增加的关系。

    * 满开始门限
      * 小于门限使用满开始
      * 大于门限，改用拥塞避免
      * 等于门限 ，两个都用

  * 拥塞避免算法
    * 让拥塞窗口缓慢变大 ，每一个RTT时间就增加1

  * 每次检测到拥塞，慢开始门限ssthresh被设置为当前窗口大小的一半（cwnd 和接收方通告窗口大小的最小值，但最少为2个报文段），然后执行慢开始算法，将拥塞窗口大小cwndn设置为1个报文段，可以迅速减少主机发送到网络的分组数，使得拥塞的路由有时间处理堆积的分组

  * 快重传
    * 每收到一个失序报文段口立即发送重复确认，不用等待发数据 在捎到。

  * 快恢复
    * 收到三个重复确认，执行乘法减半，避免拥塞
    * 接下来不执行满开始 ，而是 拥塞 避免 算法

* 描述一下TCP的三次握手的过程？
  * 三次握手前提工作
    * 在建立连接前，首先启动TCP服务器进程，服务器进程创建TCB传输控制块（存储每一个连接的信息，如TCP连接表，发送或接受缓存的指针，以及发送和接受的序号等），此时服务器就进入了LISTEN（监听）状态，时刻准备接受客户端的连接请求

  * 第一次握手：TCP客户端创建TCB连接控制块(Transmission Control Block)，接着给服务器发出一个连接请求报文段（即TCP报文的头部的同部位SYN设置为1，同时 悬着一个初始序号seq = x进行发送，该报文不能携带数据，但是也要消耗一个序号，此时TCP客户端进入了SYN-SENT（同步已发送状态）

  * 第二次握手：TCP服务器收到连接请求报文后，如果同意连接，则回发一个确认报文。确认报文中标志ACK=1，SYN=1，确认号是ack=x+1（请求报文的序号+1),同时自己也要选择一个初始序号seq=y，该报文也是不能携带数据的，也要消耗一个序号，此时，TCP服务器进程进入了SYN-RCVD（同步收到）状态。

  * 第三次握手：TCP客户收到确认后，还要向服务器给出确认。确认报文的ACK=1，ack=y+1，自己的序列号seq=x+1，此报文可以携带数据也可以不携带数据（SYN标识为1是不能携带数据），不携带数据的话不用消耗一个序号，此时，TCP连接建立，客户端进入ESTABLISHED（已建立连接）状态。

  * 当服务器收到客户端的确认后也进入ESTABLISHED状态，此后双方就可以开始通信了。

* 为什么TCP客户端最后还要发送一次确认呢？
  * 主要是为了防止重复的已经失效的连接请求报文又传到了服务器，产生了错误
  * 原因：如果使用的是二次握手，假设有这样一个场景：
    * 客户端发送了第一个连接，但由于网络拥塞，第一个连接请求报文丢失了，在规定的时间内没有收到确认报文，所以客户端进行了重传，发送了第二次连接请求报文，传输数据，然后关闭了连接，此时网络状态好了，因网络拥塞，滞留在网络上的第一个连接请求报文到了服务器，这个报文已经失效了，但是服务器却以为是新的连接到来，会再次进入连接状态，于是想客户端发送一个确认报文，并一直等待着客户端发送消息，但是客户端不会应答也不会向服务器发送数据，导致服务器空等，造成服务器资源的浪费
    * 三次握手就可以防止 这种异常的情况发生

* 描述一下TCP的四次挥手过程？
  * 数据传输后，通信双方都可以释放连接，由于 TCP 连接是全双工的，因此每个方向都必须单独进行关闭

  * 第一次挥手：客户端发出连接释放报文，并停止发送数据，主动关闭客户端到服务器的数据传输通道（此时客户端还可以接收到服务器发来的数据）,该TCP报文头部的终止控制为FIN=1，序号设置为seq = u（序号 是前面已经发送数据的最后一个字节的序号+1），此时客户端进入FIN-WAIT-1（终止等待1）状态，等待服务器的确认，FIN报文段即使不携带数据也要消耗一个序号

  * 第二次挥手：服务器接收到连接释放报文，发出确认报文，确认控制位ACK= 1，确认号ack = u＋1(收到报文的序号+1），自己的序号为v, 服务端就进入了CLOSE-WAIT（关闭等待）状态。此时，客户端到服务器的传输数据通道关闭，TCP连接处于半关闭的状态，服务器还可以把剩余的数据发送给客户端，客户端还可以接收数据，只要正确收到数据，仍应向主机B发送确认。（没有数据发送也要等待一段时间），服务器到客户端的数据传输通道 还会持续一段时间

  * 客户端接收到确认报文后，进入客户端就进入FIN-WAIT-2（终止等待2）状态，等待服务器发送连接释放报文

  * 第三次挥手：服务器在发送完最后的数据后，进程就通知TCO释放。服务器向客户端发送连接释放报文，FIN = 1,ACK = 1，ack ＝u＋1（上一次收到的序号+1），seq= w自己的 序号。此时，服务器进入最后LAST-ACK（最后确认）状态，等待客户端的确认。

  * 第四次挥手：客户端接收到服务器连接释放报文后，发送确认报文，报文的ACK控制位=1，确认号为收到的报文的序号+1 ack = w+1,自己的序号seq = u+1(第一次挥手的FIN报文序号+1，此时，进入客户端就进入了TIME-WAIT（时间等待）状态，此时TCP连接还没释放，必须经过2MSL（最长报文段寿命）的时间后，客户进入CLOSED状态。

  * 服务器接收到客户端的确认报文，立即进入COLSED状态，撤销TCB后，结束了这次的TCP连接

* MSL,TTL,RTT
  * MSL 是Maximum Segment Lifetime英文的缩写，中文可以译为“报文最大生存时间”，他是任何报文在网络上存在的最长时间，超过这个时间报文将被丢弃。因为tcp报文 （segment）是ip数据报（datagram）的数据部分，通常设置为2分钟。

  * ip头中有一个TTL域，TTL是 time to live的缩写，中文可以译为“生存时间”，这个生存时间是由源主机设置初始值但不是存的具体时间，而是存储了一个ip数据报可以经过的最大路由数，每经过一个处理他的路由器此值就减1，当此值为0则数据报将被丢弃，同时发送ICMP报文通知源主机。RFC 793中规定MSL为2分钟，实际应用中常用的是30秒，1分钟和2分钟等。
  
  * TTL与MSL是有关系的但不是简单的相等的关系，MSL要大于等于TTL

  * RTT是TCP报文客户到服务器往返所花时间（round-trip time，简称RTT），TCP含有动态估算RTT的算法。TCP还持续估算一个给定连接的RTT，这是因为RTT受网络传输拥塞程序的变化而变化

* 为什么客户端最后进入TIME_WAIT状态等待2MSL时间？

  * MSL（MAximum Segment Lifetime）是报文最长的寿命时间，它是任何报文在网络上存在的最长的最长时间，超过这个时间报文将被丢弃

  * 原因有两个
    * 第一个：为了保证客户端发送的 最后一个ACK报文段能够到达服务器，这个ACK报文可能丢失，因此使得处于最后等待确认状态的服务器收不到客户端对第三次挥手的确认。服务器会超时重传第三次握手的报文，那么A就可以在2MSL时间内收到这个重传的报文。接着客户端在重传一次确认，重新启动2MSL计时器，最后客户端和服务器都正常进入close状态 ，如果客户端不等待 2MSL时间，而是最后发送完ACK报文后立即释放连接，这样B就无法正常进入close状态。

    * 第二个:防止类似与“三次握手”中提到了的“已经失效的连接请求报文段”出现在本连接中。客户端发送完最后一个ACK报文段 后 ，再经过两个2MSL，就会使得本次连接持续的时间内产生的旧连接报文 都会从网络上消失，这样下一个新的 连接就不会出现这种旧的连接请求报文段。

  * TIME_WAIT状态为什么要等2MSL？1MSL不行吗，3MSL不行吗？
    * 如果是为了防止已失效的连接请求报文段出现在下一次连接，只需要1MSL就可以了，因为一般MSL都会大于ip报文的TTL，1MSL时间后所有失效报文都会被丢弃。
    * 设置为2MSL的原因是原因1
      * 现在假设服务器发完所有数据了，发出了第三次握手的确认报文，由于网络不好，这个报文成了极限报文（或者丢失了)，用了1MSL后才能到达客户端，客户端回应一个第四次握手的确认报文（此时服务器已经等了1MSL时间了），然后进入timeout，只能重传第三次握手的报文 （而此时客户端已经在time_wait状态等了1MSL,第四次握手的报文才刚出门），如果网络很慢，那么第四次握手的报文也用了1MSL到达了服务端，客户端的time_wait也接近2MSL，即将关闭，所以time_wait时间不能小于2MSL，如果此时客户端收到服务器重传的第三次握手报文会重置time_wait的等待时间为2MSL。
      * 那为什么不是3MSL或者更多？是可以设置但没必要，也没有意义，因为第三次握手重传的timeout时间不会出超过2MSL，如果客户端在2MSL时间还没有收到重传的报文，那么说明服务器已经接收了第四次握手的确认报文，并且关闭了TCP连接，等下去也是没有意义的 
      * 简而言之，就是客户端四次挥手后，发出的确认报文如果正常被服务器接收，服务器就会进入关闭状态，但是如果报文由于拥塞原因没送到，这时服务器就会timeout重发三次握手的报文，重发报文会在2MSL时间内到达客户端，这时客户端就会认为服务器还没收到第四次挥手的报文，将time_wait状态重置为2MSL,再次等待。

* 为什么建立连接是三次握手，关闭连接确是四次挥手呢？
  * 建立连接的时候，服务器在LISTEN状态下，收到建立连接请求SYN报文，把确认序号ACK和同步序号SYN放在一个报文里发送给客户端，其中ACK报文是用来应答的，SYN报文是用来同步的
  * 而关闭连接时，服务器收到了客户端的连接释放FIN报文，很可能并不会立即关闭SOCKET，先发送确认报文（ACK=1)关闭客户端发送数据给服务器的通道，服务器可能还有数据没发送完，所以服务器可以将剩余的数据发送完，再发送连接释放FIN(FIN=1,ACK=1)报文给对方表示同意关闭连接，因此服务器的确认报文和连接释放报文是分来发送的，从而多了一次挥手过程

* 如果已经建立了连接，但是客户端突然出现故障了怎么办？
  * TCP还设有一个保活计时器，显然，客户端如果出现故障，服务器不能一直等下去，白白浪费资源。服务器每收到一次客户端的请求后都会重新复位这个保活计时器，时间通常是设置为2小时，若两小时还没有收到客户端的任何数据，服务器就会发送一个探测报文段，以后每隔75分钟发送一次。若一连发送10个探测报文仍然没反应，服务器就认为客户端出了故障，接着就关闭连接。

* tcp粘包问题
  * 发送端需要等缓冲区满才发送出去，造成粘包
  * 接收方不及时接收缓冲区的包，造成多个包接收,从而造成了粘包问题
  * 解决办法
    * 为了避免粘包现象，可采取以下几种措施。
      * 一是对于发送方引起的粘包现象，用户可通过编程设置来避免，TCP提供了强制数据立即传送的操作指令push，TCP软件收到该操作指令后，就立即将本段数据发送出去，而不必等待发送缓冲区满
      * 发送方也可以自定义结构类型去存储要发送的数据，再根据结构大小封装保，那么接收方也可以根据结构体大小去区分粘在一起的不同的数据报文。
      * 二是对于接收方引起的粘包，则可通过优化程序设计、精简接收进程工作量、提高接收进程优先级等措施，使其及时接收数据，从而尽量避免出现粘包现象

* socket选项
  * Socket选项就是为满足用户的定制化需求而生的。我们经常遇到的情况包括地址复用、端口复用、读写超时时间、读写缓冲区大小等
  * 读取和设置Socket选项的API包括:getsockopt、setsockopt、fcntl、ioctl等；
  * 选项
    * SO_REUSEADDR选项
      * 一般来说，一个端口释放后会等待两分钟之后才能再被使用，SO_REUSEADDR是让端口释放后立即就可以被再次使用
      * 场景
        * 当有一个有相同本地地址和端口的socket1处于TIME_WAIT状态时，而你启动的程序的socket2要占用该地址和端口，你的程序就要用到该选项。
    * TIME_WAIT
      * TIME_WAIT状态是TCP协议为了保证全双工连接可靠性设置的
        * 可靠地实现TCP全双工连接的终止
        * 可靠地实现TCP全双工连接的终止
    * SO_REUSEPORT选项
      * 作用效果:端口复用选项
      * 支持多个进程或者线程绑定到同一端口，提高服务器程序的性能
        * )单进程或线程创建socket，并进行listen和accept，接收到连接后创建进程和线程处理连接
        * 单进程或线程创建socket，并进行listen，预先创建好多个工作进程或线程accept()在同一个服务器套接字
    * TCP_NODELAY选项
      * 启用Naggle算法被引入到协议栈，算法思想尽可能发送大块数据，避免网络中充斥着许多小数据块，任意时刻最多只能有一个未被确认的小段。未被确认是指一个数据块发送出去后，没有收到对方发送的ACK确

* 为什么报文有最大生存时间？
  * 任何报文在网络上存在的最长时间，超过这个时间，报文就会被丢弃，一个ip报文头部有一个TTL的生存时间，表示数据报文可以经过的最大路由数，当值为0是数据报文就会被丢弃，并法ICMP报文给源主机。
* 这个最大生存时间是由什么决定的（在网络传输中的哪一层及哪个设备决定）？
  * 由网络层的路由器决定的。

* TCP短连接和长连接
  * TCP短连接
    * 流程
      * client 向 server 发起连接请求
      * server 接到请求，双方建立连接
      * client 向 server 发送消息
      * server 回应 client
      * 一次读写完成，此时双方任何一个都可以发起 close 操作
      * 短连接一般只会在 client/server 间传递一次读写操作！
    * 特点
      * 管理起来比较简单，存在的连接都是有用的连接，不需要额外的控制手段

  * tcp长连接
    * 流程
      * client 向 server 发起连接
      * server 接到请求，双方建立连接
      * client 向 server 发送消息
      * server 回应 client
      * 一次读写完成，连接不关闭
      * 后续读写操作

    * TCP保活功能
      * 保活功能主要为服务器应用提供，服务器应用希望知道客户主机是否崩溃，从而可以代表客户使用资源。
      * 如果客户已经消失，使得服务器上保留一个半开放的连接，而服务器又在等待来自客户端的数据，保活功能就是试图在服务器端检测到这种半开放的连接

      * 如果一个给定的连接在2小时内没有任何的动作，则服务器就向客户发一个探测报文段，客户主机必须处于以下4个状态之一：
        * 客户主机依然正常运行，并从服务器可达。客户的TCP响应正常，而服务器也知道对方是正常的，服务器在两小时后将保活定时器复位。
        * 客户主机已经崩溃，并且关闭或者正在重新启动。在任何一种情况下，客户的TCP都没有响应。服务端将不能收到对探测的响应，并在75秒后超时。服务器总共发送10个这样的探测 ，每个间隔75秒。如果服务器没有收到一个响应，它就认为客户主机已经关闭并终止连接。
        * 客户主机崩溃并已经重新启动。服务器将收到一个对其保活探测的响应，这个响应是一个复位，使得服务器终止这个连接。
        * 客户机正常运行，但是服务器不可达，这种情况与2类似，TCP能发现的就是没有收到探查的响应。
  * 优缺点
    * 长连接
      * 长连接可以省去较多的TCP建立和关闭的操作，减少浪费，节约时间
      * 对于频繁请求资源的客户来说，较适用长连接
      * 存在一个问题，存活功能的探测周期太长，还有就是它只是探测TCP连接的存活，遇到恶意的连接时，保活功能就不够使了，随着客户端连接越来越多，server早晚有扛不住的时候，这时候server端需要采取一些策略，如关闭一些长时间没有读写事件发生的连接，这样可以避免一些恶意连接导致server端服务受损
    * 短连接
      * 管理较为简单，存在的连接都是有用的连接，不需要额外的控制手段。
      * 如果客户请求频繁，将在TCP的建立和关闭操作上浪费时间和带宽

  * 应用场景
    * 长连接/短链接的应用场景
      * 长连接
        * 长连接多用于操作频繁，点对点的通讯，而且连接数不能太多情况。每个TCP连接都需要三步握手，这需要时间，如果每个操作都是先连接，再操作的话那么处理速度会降低很多，所以每个操作完后都不断开，再次处理时直接发送数据包就OK了，不用建立TCP连接。
        * 例如：数据库的连接用长连接，如果用短连接频繁的通信会造成socket错误，而且频繁的socket 创建也是对资源的浪费。
      * 短链接
        * 而像WEB网站的http服务一般都用短链接，因为长连接对于服务端来说会耗费一定的资源，而像WEB网站这么频繁的成千上万甚至上亿客户端的连接用短连接会更省一些资源，如果用长连接，而且同时有成千上万的用户，如果每个用户都占用一个连接的话，那可想而知吧。所以并发量大，但每个用户无需频繁操作情况下需用短连好。

* 基于tcp的应用层协议中，哪个字段最重要
  * (答案是长度，因为tcp是流传输协议)

* tcp协议的MTU和MSS
  * mtu 最大传输单元，以太网的MTU是1500
  * MSS就是TCP数据包每次能够传输的最大数据分段
    * 为了达到最佳的传输效能，TCP协议在建立连接的时候通常要协商双方的MSS值，这个值TCP协议在实现的时候往往用MTU值代替（需要减去IP数据包包头的大小20Bytes和TCP数据段的包头20Bytes）所以往往MSS为1460。通讯双方会根据双方提供的MSS值为最小值确定为这次连接的最大MSS值。

* socket阻塞与非阻塞，同步与异步
  * 同步/异步主要是针对client端
    * 同步：就是在client端发出一个功能调用时，在没有得到结果之前，该调用就不返回
    * 异步：与同步相对，当client端调用发出后，会立即返回，但调用者不能立刻得到结果，而是由其它通知或回调来通知调用者。
  * 阻塞/非阻塞主要针对server端
    * 阻塞调用是指调用结果返回之前，当前线程会被挂起，函数只有在得到结果之后才会返回。
    * 非阻塞则会立刻返回。像Recv接口是阻塞的，RecvFrom非阻塞的。

* 静态路由和动态路由的理解（深信服）
  * 静态路由是指由网络管理员手工配置的路由信息。当网络的拓扑结构或链路的状态发生变化时，网络管理员需要手工去修改路由表中相关的静态路由信息。静态路由信息在缺省情况下是私有的，不会传递给其他路由器
    * 静态路由一般适用于比较简单的网络环境
    * 使用静态路由的另一个好处是网络安全保密性高

  * 动态路由器上的路由表项是通过相互连接的路由器之间交换彼此信息，然后按照一定的算法优化出来的，而这些路由信息是在一定时间间隙里不断更新，以适应不断变化的网络，以随时获得最优的寻路效果

* 常见的路由协议？
  * 内部网关协议
    * RIP（Routing Information Protocol）：路由信息协议
      * 主要用于规模较小的网络，RIP是一种基于距离矢量（Distance-Vector）算法的协议，它通过UDP报文进行路由信息的交换，使用的端口号为520。其使用跳数来衡量到达目的地址的距离，为了限制收敛时间，RIP规定度量值（该值等于从本网络到达目的网络间的路由器数量）为0到15之间的整数，大于等于16的跳数将会定义为网络或主机不可达，因此RIP不适合大型网络。
      * OSPF（Open Shortest Path First）：开放式最短路径优先协议。
        * 属于链路状态路由协议，OSPF提出了“区域（area）”的概念，每个区域中所有路由器维护着一个相同的链路状态数据库 （LSDB），其使用链路状态数据库，通过最短生成树算法（SPF算法）计算得到路由表，因此其收敛速度较快。

* 什么是网关？
  * 网关(Gateway)就是一个网络连接到另一个网络的“关口”，在传输层上以实现网络互连，用于两个高层协议不同的网络互连，在使用不同的通信协议、数据格式或语言，甚至体系结构完全不同的两种系统之间，网关是一个翻译器

* ARP协议过程？
  * 同一个网络下：
    * 主机A查看自己的ARP缓存，检查是否有主机B的IP到MAC的映射，如果有映射，构造报文，目的IP为主机B的IP，源IP为主机A的IP，目的MAC为主机B的MAC，源MAC为主机A的MAC，将报文发送给交换机C，
      * 交换机C进行MAC地址表学习，将主机A的MAC和报文入端口号记录下来，然后交换机C查看自己的MAC转发表，检查是否有主机B的MAC到端口的映射，如果有映射，获取对应的端口，将报文从此端口转发出去，报文到达主机B。如果交换机C没有主机B的MAC转发表映射，采用洪泛的形式广播报文，主机B收到报文后向主机A回复，交换机C进行MAC表学习，将主机B的MAC和报文入端口号记录下来。
    * 如果主机A没有主机B的ARP映射，主机A需要发送ARP请求，以获取主机B的MAC，将报文发往交换机C，交换机C采用洪泛的形式广播报文，主机B收到广播报文后，在自己的ARP缓存表中写入主机A的IP到MAC的映射，将自己的MAC封装到ARP回复报文中，单播给主机A，主机A获取到主机B的MAC后，在自己的ARP缓存表中写入主机B的IP到MAC的映射，构造报文发送给主机B，过程同上。

  * 不同网络下：
    * 主机 A 会首先检查目的IP地址是否与自己在同一网段，如果在，就直接广播ARP请求来获取目的主机的MAC地址，如果不在同一网段，又配置有网关地址的话，那么主机 A 就通过 ARP 请求，询问192.168.0.1（网关）在哪里，网关收到后就会回应主机 A ，把网关的MAC地址告诉主机 A ，当获取到网关的MAC地址后，把网关的MAC地址作为MAC帧中的目的MAC地址，然后就把数据丢给网关 192.168.0.1 ，网关根据路由表，转发给下一个路由器，再由下一个路由器交付给主机 D 所在的网络，即网关，网关再通过ARP，找到目的主机 D ，完成数据交付。

### 应用层

* 网络编程?
  * 网络编程的本质是两个设备之间的数据交互，设备主要指的是计算机，数据交互的意思就是把一个设备的数据发给另外一个设备，然后接收另一个设备反馈的数据，现在的网络编程基本都是基于都是C/S模型。
  * 发起连接的程序，称为客户端，在需要的时候启动；等待客户端连接的程序程序称为服务器，服务器为了时刻接收连接，需要一直启动，被动接收客户端的连接。
  * 在数据交互之前，服务器与客户端需要通过一些IP和端口建立连接，ip对应网络上的具体的计算机，端口对应该计算机上进行连接的程序，连接建立后就可以数据交互
  * 数据交互通过一些网络传输协议完成，即一些客户端和服务器双方统一的传输数据格式。
  * 总而言之，网络编程就是使用ip或域名，和端口连接到网络上另一台计算机上的程序，按照规定的协议（传输的数据的格式）来交换数据，实际编程中建立连接，收发数据已经在语言级别实现，主要的工作就是设计协议，以及编写生成数据和解析数据的代码。

* HTTT协议
  * http请求报文有三部分：
    * 由请求行，请求头，请求数据三部分组成。
  * 请求行包括请求方法、URL和HTTP协议版本。请求方法有GET,POST,HEAD等等。
  * 请求头：由关键字/值对组成，每行一对，关键字和值用冒号隔开，请求头是通知服务端有关于客户端请求的信息。如Host:请求的主机名
  * 请求数据：用于POST方法中。
  * GET：客户端要从服务端读取数据时用GET，使用GET方法时，请求参数和对应的值以明文的方式附加在URL后面，利用一个问号（？）代表URL的结尾和请求参数的开始，传递参数长度受限制，例：/index.jsp?id=100&op=bind
  * POST:是向服务器提交数据，POST方法请求参数封装在HTTP请求数据中，可以传输大量数据，可用来传送文件。

* 在浏览器输入 URL 回车后，会发生什么？
  * 简介
    * URL 解析
    * DNS 查询
    * TCP 连接
    * 处理请求
    * 接受响应
    * 渲染页面
  * URL解析
    * 地址解析：判断输入的内容是URL还是一个待搜索的关键字，并且根据你输入的内容进行自动完成字符编码等操作
    * 由于安全隐患，一般强制客户端使用https访问页面
  * DNS查询
    * 查看浏览器缓存中有没有目标url
    * 操作系统也有自己的 DNS缓存，本地的 Hosts 文件
    * 查看路由器缓存
    * 查看 本地DNS 服务器
    * 在前面所有步骤没有缓存的情况下，本地 DNS 服务器会将请求转发到互联网上的根域
  * TCP 连接
    * TCP/IP 分为四层（http,tcp,ip,数据链路层），在发送数据时，每层都要对数据进行封装
  * 服务器处理请求
    * 接受 TCP 报文后，会对连接进行处理，对HTTP协议进行解析（请求方法、域名、路径等），并且进行一些验证
    * 假如服务器配置了 HTTP 重定向，就会返回一个 301永久重定向响应，浏览器就会根据响应，重新发送 HTTP 请求（重新执行上面的过程）。
  * 浏览器接受响应
    * 浏览器接收到来自服务器的响应资源后，会对资源进行分析。
  * 浏览器渲染页面

* GET与POST的区别
  * Get是向服务器索取数据的一种请求，而Post是向服务器提交数据的一种请求。Get请求的参数会跟在url后进行传递,它对传输的数据有大小限制。POST请求的数据会放置在HTML Header内提交。Post比Get安全，当数据是中文或者不敏感的数据，则用get，因为使用get，参数会显示在地址上，对于敏感数据和不是中文字符的数据则用POST。

* HTTP状态码：表示网页服务器HTTP响应状态的3位数字代码。一般状态码的第一个数字代表了响应的五种状态。
  * 比如2XX开头的代表成功，像200代表请求已成功，表示正常状态。202代表服务器已接受请求，但尚未处理。
  * 3XX代表重定向，需要客户端采取进一步操作才能完成请求。
  * 4XX代表请求错误，401请求身份验证，403表示拒绝执行，404表示请求失败。
  * 5开头代表服务器错误。

* 在浏览器中输入URL后，执行的全部过程。会用到哪些协议？（一次完整的HTTP请求过程）
  * 根据输入的内容获取到URL
  * 域名解析
    * 先看浏览器缓存，本地操作系统缓存DNS韩村，再hosts文件，再到本地DNS服务器，本地服务器再向根域服务器，找到目的主机的ip地址
  * 发送消息之前需要通过三次握手去建立连接
    * 这个过程首先会用到ARP地址解析协议，根据目的主机的IP获取到对方以太网mac地址
      * 先到arp缓存中找，没有则 向局域网发送arp请求报文，目的主机接受到该报文，会发送自己的mac地址到arp响应包
    * 网络层主要做的是通过查找路由表确定如何到达服务器，用到路由选择协 ospf
  * 建立三次握手后可以就可以使用http协议发送http请求到网站
  * 服务器响应HTTP请求
  * 浏览器解析html代码，并请求HTML代码中的资源
  * 断开TCP连接
  * 浏览器对页面进行渲染呈现给用户

* http如何变成https
  * http网站是无状态的，在传送数据的时候也没有对数据进行任何形式的加密，更不会对客户端和服务器进行反复的验证。这样在传送数据的时候，很可能被黑客利用，通过域名劫持等技术手段返回虚假的网站或数据给客户端。造成用户隐私泄露或财产损失
    * 被窃听的风险：第三方可以截获并查看你的内容
    * 被篡改的危险：第三方可以截获并修改你的内容
    * 被冒充的风险：第三方可以伪装成通信方与你通信

  * https比http协议更加安全，因为它采用了SSL安全机制进行通信，在http上加了SSL/TLS协议，以数字证书为基础，对传输的数据进行加密，让网站的信息在传输过程中当时不会被截取和篡改。在数据传送的过程会对客户端和浏览器进行反复的验证，确保通过域名可以访问到你想要访问的那台服务器，从而大大减少网站被劫持、被镜像的风险

  * HTTPS 在内容传输的加密上使用的是对称加密，非对称加密只作用在证书验证阶段
    * 对称加密，即采用对称的密码编码技术，他的特点是，加密和解密使用相同的秘钥。
    * 非对称加密技术，需要两个秘钥，公钥和私钥。公钥和私钥成对出现。

  * 原理
    * 第一步是证书验证
      * 浏览器发起http请求，访问服务器
      * 服务器接受请求，把数字证书+公用密匙 发回给客户端
      * 浏览器验证服务器证书，确保访问的是正确的服务器（不是钓鱼网站）
    * 第二步是数据传输阶段
      * 浏览器生产随机数作为会话密匙，并用公用密匙进行加密再次发给服务器
      * 服务器用私人密匙进行对随机数进行解密（也就相当于验证客户端），通过随机数构造对称加密算法，建立起一条安全的数据传递通道
      * 服务器把客户端请求的数据打包加密发送给客户端->客户端浏览器接收数据并解析。

  * 为什么数据传输要用对称加密
    * 非对称加密的加密/解密效率低下，http的应用场景通常有着大量的交互，非对称加密的效率是无法接受的
    * https的场景中只有服务端保存了私钥，一对公私钥只能实现单向的加密解密，所以https的数据传输采用了对称加密

* 为什么需要 CA 认证机构颁发证书
  * HTTP 协议被认为不安全是因为传输过程容易被监听者勾线监听、伪造服务器，而 HTTPS 协议主要解决的便是网络传输的安全性问题。
  * 如果不存在认证机构，任何人都可以制作证书，这带来的安全风险便是经典的“中间人攻击”问题

* 证书包含什么信息？
1）颁发机构信息；
2）公钥；
3）公司信息；
4）域名；
5）有效期；
6）指纹；
7）......

* 本地随机数被窃取怎么办？
  * 证书验证是采用非对称加密实现，但是传输过程是采用对称加密，而其中对称加密算法中重要的随机数是由本地生成并且存储于本地的，HTTPS 如何保证随机数不会被窃取？
  * HTTPS 并不包含对随机数的安全保证，HTTPS 保证的只是传输过程安全，而随机数存储于本地，本地的安全属于另一安全范畴，应对的措施有安装杀毒软件、反木马、浏览器升级修复漏洞等

* 用了 HTTPS 会被抓包吗？
  * HTTPS 的数据是加密的，常规下抓包工具代理请求后抓到的包内容是加密状态，无法直接查看。

* 对称加密和非对称加密
  * 对称加密: 加密和解密的秘钥使用的是同一个
  * 非对称加密: 与对称加密算法不同，非对称加密算法需要两个密钥：公开密钥（publickey）和私有密钥（privatekey）
    * 非对称加密算法实现机密信息交换的基本过程是：甲方生成一对密钥并将其中的一把作为公用密钥向其它方公开；得到该公用密钥的乙方使用该密钥对机密信息进行加密后再发送给甲方；甲方再用自己保存的另一把专用密钥对加密后的信息进行解密。甲方只能用其专用密钥解密由其公用密钥加密后的任何信息。

* ping过程（深信服）
  * PING (Packet Internet Groper)，因特网包探索器，用于测试网络连通性的程序，ICMP报文
  * 利用网络上机器IP地址的唯一性，给目标IP地址发送一个数据包，再要求对方返回一个同样大小的数据包来确定两台网络机器是否连接相通，时延是多少。
  * 比如主机A要访问主机B，访问之前要ping一下判断网络是否连通
  * 如果连个主机是同一个网段
    * 首先它需要知道那个目标主机的mac地址（mac缓存表没有则向外广播一个ARP包，等待对方发送回应包
    * 主机A就知道主机B的amc地址，将这个mac地址封装到ICMP协议的二层报文中向主机B发送
    * 目的主机B接收到这个报文之后，发现是ICMP的回显请求，就以同样的格式返回一个数据包
    * 主机A接受到数据包后，则表明两个主机之间的网络是连通的

  * 两个主机是不同的网段
    * 主机A要ping主机C，那么主机A发现主机C的IP和自己不是同一网段，他就去找网关转发，如果ARP缓存没有则通过发送ARP广播包寻找
    * 然后继续按照之前的不步骤找到目的主机的mac地址，封装一个ICMP报文发送给网关
    * 当路由器接收到主机A 发送的 ICMP报文，就会根据自己的路由表，根据映射表将源mac替换为路由器本身的mac地址，然后向 主机 C转发，如果没有主机C的mac地址，同样 通过ARP协议来学习主机C的mac地址
    * 主机C顺序接受到主机A的ICMP包，然后回应一个相同格式ICMP包
    * ICMP包也是以相同的方式通过路由转发回主机A
    * 主机A接受到ICMP包后表示网络连通

* Bind error， address already 被 use，如何解决(深信服)
  * 端口被占用
    * lsof -i :10080命令往往又看不到哪个进程占用了端口
    * netstat -ano来显示协议统计信息和TCP/IP网络连接
    * 如果是无关进程则根据进程号，将其kill

### I/O多路���用机制

* IO多路复用是指内核一旦发现进程指定的一个或者多个IO条件准备读取，它就通知该进程。IO多路复用适用如下场合：

　　（1）当客户处理多个描述字时（一般是交互式输入和网络套接口），必须使用I/O复用。

　　（2）当一个客户同时处理多个套接口时，而这种情况是可能的，但很少出现。

　　（3）如果一个TCP服务器既要处理监听套接口，又要处理已连接套接口，一般也要用到I/O复用。

　　（4）如果一个服务器即要处理TCP，又要处理UDP，一般要使用I/O复用。

　　（5）如果一个服务器要处理多个服务或多个协议，一般要使用I/O复用

* 与多进程和多线程技术相比，I/O多路复用技术的最大优势是系统开销小，系统不必创建进程/线程，也不必维护这些进程/线程，从而大大减小了系统的开销。

* 多路复用有三种方式
  * select
  * poll
  * epoll

* 三种IO复用应用场景和区别？
  * IO多路复用的本质时通过一种机制，复用的是进程，让单个进程可以监视多个文件描述符，一旦某个描述符就绪，能够通知程序进行相应的读写操作。
  * select：
    * select()的机制中提供一种fd_set的数据结构,就是一个维护了socket的数组
    * select:在网络编程中统一的操作顺序是创建socket－>绑定端口－>监听－>accept->write/read,当有客户端连接到来时,select会把该连接的文件描述符放到fd_set集合中每次调用select，都需要把fd_set集合从用户态拷贝到内核态，如果fd_set集合很大时，那这个开销也很大，内核对被监控的fd_set集合大小做了限制，并且这个是通过宏控制的，大小不可改变(限制为1024)。除此之外在每次遍历这些描述符之前，系统还需要把这些描述符集合从用户空间copy到内核，然后再把有事件到达的文件描述符copy回去，如果此时没有一个描述符有事件发生，这些copy操作和遍历操作都是无用功，可见slect随着连接数量的增多，效率大大降低。可见如果在高并发的场景下select并不适用

  * poll的机制与select类似，与select在本质上没有多大差别，管理多个描述符也是进行轮询，根据描述符的状态进行处理，但是poll没有最大文件描述符数量的限制。poll和select同样存在一个缺点就是，包含大量文件描述符的数组被整体复制于用户态和内核的地址空间之间，而不论这些文件描述符是否就绪，它的开销随着文件描述符数量的增加而线性增大。

  * epoll: epoll是基于事件驱动的IO方式，它的优点就是在于它的效率和并发量。相对于select来说，epoll没有描述符个数限制，使用一个文件描述符管理多个描述符，将用户关心的文件描述符事件存放在内核的一个事件表中，这样用户空间和内核空间的copy只需一次,在内核中是以红黑树节点去存储的。
    * 提高程序在大量并发连接中只有少量活跃的情况下的系统CPU利用率，获取事件的时候，它无须遍历整个被侦听的描述符集，只要遍历那些被内核IO事件异步唤醒而加入Ready队列的描述符集合
    * int epoll_create(int size);
      * 创建一个epoll的句柄，内核帮我们在epoll文件系统里建了个file结点，size用来告诉内核这个监听的数目一共有多大。这个参数不同于select()中的第一个参数，给出最大监听的fd+1的值。需要注意的是，当创建好epoll句柄后，它就是会占用一个fd值，在linux下如果查看/proc/进程id/fd/，是能够看到这个fd的，所以在使用完epoll后，必须调用close()关闭，否则可能导致fd被耗尽。
    * epoll_ctl 的事件注册函数，注册要监听的事件类型
      * 内核用红黑树来存储以后epoll_ctl传来的socket id，当有新的socket连接来时，先遍历红黑书中有没有这个socket存在，如果有就立即返回，没有就插入，然后给内核中断处理程序注册一个回调函数，每当有事件发生时就通过回调函数把这些文件描述符放到事先准备好的用来存储就绪事件的链表中
    * epoll_wait时，会把准备就绪的socket 描述符链表拷贝到用户态内存，然后清空list链表，最后检查这些socket

    * epoll的优势非常明显，几乎没有描述符数量的限制，并发支持完美，不会随着socket的增加而降低效率，也不用在内核空间和用户空间之间做无效的copy操作

  * 两种触发方式
    * 水平触发（LT）：默认工作模式，即当epoll检测到某描述符事件就绪并通知应用程序时，应用程序可以不立即处理该事件；下次调用时，会再次通知此事件,如果这些socket上确实有未处理的事件时，该句柄会再次被放回到刚刚清空的准备就绪链表，保证所有的事件都得到正确的处理，
      * 如果到timeout时间后链表中没有数据也立刻返回，因此在并发需求量高的场景中我们即使要监控数百万计的句柄，大多数一次也只返回很少量的准备就绪句柄。由此可见epoll仅需要从内核态copy少量的句柄到用户态，这样就

    * 边缘触发（ET）： 当epoll检测到某描述符事件就绪并通知应用程序时，应用程序必须立即处理该事件。如果不处理，下次调用不会再次通知此事件。（直到你做了某些操作导致该描述符变成未就绪状态了，也就是说边缘触发只在状态由未就绪变为就绪时只通知一次）
      * 当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序必须立即处理该事件。如果不处理，下次调用epoll_wait时，不会再次响应应用程序并通知此事件。

    * 比较
      * 相比较而与，边缘触发减少了事件的触发次数，效率更高一些。但是需要用户及时去处理事件。

  * epoll的优势非常明显，几乎没有描述符数量的限制，并发支持完美，不会随着socket的增加而降低效率，也不用在内核空间和用户空间之间做无效的copy操作
  * 在并发量低，socket都比较活跃的情况下，select的性能不会比epoll差，而且不用像epoll一样再内核中维护一个红黑树去管理socket
  * 但是当并发量较大时，epoll是Linux目前大规模网络并发程序开发的首选模型，目前流行的高性能web服务器Nginx正式依赖于epoll提供的高效网络套接字轮询服务
  
  * 应用场景：一个游戏服务器，tcp server负责接收客户端的连接，dbserver负责处理数据信息，一个webserver负责处理服务器的web请求，gameserver负责游戏的逻辑处理，所有这些服务都和另外一个gateserver相连，gateserver负责服务器间的通信和转发（进程间通信），只要游戏服务器在服务状态，这些连接几乎不会断开（异常情况可能会断开），并且这些连接数量一般不会很多。这种情况使用select，因为每时每刻这些连接的socket都有事件发生（比如：服务期间的心跳信息，还有大型网络游戏的同步信息（一般每秒在20-30次）），最重要的是，这种场景下，并发量也不会很大。如果此时用epoll，为此所建立的文件系统，红黑书和链表对于此来说就是杀鸡用牛刀，效率反而不  高。当然这里的tcp server负责大量的客户端的连接，毫无疑问epoll是首选，它接受大量的客户端连接，收到客户端的消息之后把消息转发发给select网络模型的gateserver，gateserver再转发给gameserver进行逻辑处理，最后返回给客户端就over了。因此在如果在并发量低，socket都比较活跃的情况下，select就不见得比epoll慢了(就像我们常常说快排比插入排序快，但是在特定情况下这并不成立)

  * 应用场景
    * 在LT模式下，无论fd是否有事件发生，或者还有一些事件没有处理完，每次调用epoll_wait时，总会得到该fd让你处理（只要有没事件没有处理，会一直通知你处理，直到你处理完为止，这样就保证了数据的不丢失）。

    * 在ET模式下，当有事件发生时，系统只会通知你一次，即在调用epoll_wait返回fd后，不管这个事件你处理还是没处理，处理完没有处理完，当再次调用epoll_wait时，都不会再返回该fd，这样的话程序员要自己保证在时间发生时要及时有效的处理完该事件。

    * 如果是并发量比较大，而且每个连接通信的数据量比较大的情况，为了不饥饿掉其他连接，采用LT触发模式注册可读事件，通信框架中限制每个连接接受的最大数据为1M，假设一个连接中总共会有5M，那当可读事件触发时，就只会读取1M，然后把这份数据缓存在应用层，然后处理其他连接后再回来处理接下来的第二个1M，重复上述逻辑，知道5M数据都取完毕，再把这些数据批量抛给业务逻辑；这样保证了每个业务的吞吐量；2.如果对每个用户实时性要求比较高，就每次尽最大努力读完5M数据后再处理其他可读事件

* 使用Linux epoll模型，水平触发模式；当socket可写时，会不停的触发 socket 可写的事件，如何处理？
  * 第一种最普遍的方式：需要向 socket 写数据的时候才把 socket 加入 epoll ，等待可写事件。接受到可写事件后，调用 write 或者 send 发送数据。当所有数据都写完后，把 socket 移出 epoll。这种方式的缺点是，即使发送很少的数据，也要把 socket 加入 epoll，写完后在移出 epoll，有一定操作代价。
  * 开始不把 socket 加入 epoll，需要向 socket 写数据的时候，直接调用 write 或者 send 发送数据。如果返回 EAGAIN，把 socket 加入 epoll，在 epoll 的驱动下写数据，全部数据发送完毕后，再移出 epoll。这种方式的优点是：数据不多的时候可以避免 epoll 的事件处理，提高效率

* 使用Linux epoll模型，水平触发模式；当socket可写时，会不停的触发 socket 可写的事件，如何处理？
  * 第一种最普遍的方式：需要向 socket 写数据的时候才把 socket 加入 epoll ，等待可写事件。接受到可写事件后，调用 write 或者 send 发送数据。当所有数据都写完后，把 socket 移出 epoll。这种方式的缺点是，即使发送很少的数据，也要把 socket 加入 epoll，写完后在移出 epoll，有一定操作代价。
  * 开始不把 socket 加入 epoll，需要向 socket 写数据的时候，直接调用 write 或者 send 发送数据。如果返回 EAGAIN，把 socket 加入 epoll，在 epoll 的驱动下写数据，全部数据发送完毕后，再移出 epoll。这种方式的优点是：数据不多的时候可以避免 epoll 的事件处理，提高效率